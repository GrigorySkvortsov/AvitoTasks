{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AvitoTask\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Фикс: в новых версиях Python inspect.getargspec отсутствует\n",
    "import inspect\n",
    "from collections import namedtuple\n",
    "\n",
    "if not hasattr(inspect, \"getargspec\"):\n",
    "    ArgSpec = namedtuple('ArgSpec', 'args varargs keywords defaults')\n",
    "    def getargspec(func):\n",
    "        spec = inspect.getfullargspec(func)\n",
    "        return ArgSpec(spec.args, spec.varargs, spec.varkw, spec.defaults)\n",
    "    inspect.getargspec = getargspec\n",
    "\n",
    "# Основные библиотеки\n",
    "import pandas as pd   # для работы с таблицами\n",
    "import math           # для логарифмов и мат. операций\n",
    "import pymorphy2      # морфологический анализатор для русского\n",
    "from transformers import AutoTokenizer  # WordPiece токенизатор\n",
    "import re             # регулярные выражения\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AvitoTask\\.venv\\Lib\\site-packages\\pymorphy2\\analyzer.py:114: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MorphAnalyzer и токенизатор загружены\n"
     ]
    }
   ],
   "source": [
    "# Инициализация инструментов\n",
    "morph = pymorphy2.MorphAnalyzer()  # морфологический анализатор для распознавания слов и форм\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")  # компактный WordPiece токенизатор\n",
    "\n",
    "print(\"MorphAnalyzer и токенизатор загружены\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружено 1423050 слов в словарь\n"
     ]
    }
   ],
   "source": [
    "# Загрузка частотного словаря ru_full.txt\n",
    "word_freqs = {}\n",
    "with open(\"ru_full.txt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) == 2:\n",
    "            word, freq = parts\n",
    "            word_freqs[word.lower()] = int(freq)\n",
    "\n",
    "# Преобразуем частоты в логарифмы, чтобы снизить большие различия\n",
    "log_freqs = {w: math.log(f) for w, f in word_freqs.items()}\n",
    "print(f\"Загружено {len(word_freqs)} слов в словарь\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_score(word: str) -> float:\n",
    "    \"\"\"\n",
    "    Оценивает слово по pymorphy2 и частотному словарю\n",
    "    Чем выше score, тем более правдоподобно, что сегмент является словом\n",
    "    \"\"\"\n",
    "    # Наказываем слишком короткие сегменты, чтобы избежать чрезмерного дробления текста\n",
    "    if len(word) < 2:\n",
    "        return -1.2  \n",
    "    elif len(word) < 3:\n",
    "        return -0.8  \n",
    "    elif len(word) < 4:\n",
    "        return -0.3  \n",
    "\n",
    "    parses = morph.parse(word)\n",
    "    score = 0.0\n",
    "\n",
    "    # Учитываем морфологический разбор pymorphy2\n",
    "    if parses:\n",
    "        best = parses[0]  # берём наиболее вероятный разбор\n",
    "        prob = best.score if best.score > 0 else 1e-6\n",
    "        score += math.log(prob + 1e-9)\n",
    "\n",
    "    # Учитываем частотный словарь: часто встречающиеся слова получают бонус\n",
    "    if word.lower() in log_freqs:\n",
    "        score += log_freqs[word.lower()]\n",
    "\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(text: str):\n",
    "    \"\"\"\n",
    "    Делит текст на слова с помощью динамического программирования\n",
    "    Ищем разбиение с максимальной суммарной оценкой (get_word_score)\n",
    "    \"\"\"\n",
    "    n = len(text)\n",
    "    # dp[i] = (лучший score до позиции i, индекс начала последнего слова)\n",
    "    dp = [(-1e12, -1)] * (n + 1)\n",
    "    dp[0] = (0.0, -1)  # стартовое условие\n",
    "\n",
    "    # Основной цикл: для каждой позиции ищем лучшее разбиение\n",
    "    for i in range(1, n + 1):\n",
    "        best_score, best_j = -1e12, -1\n",
    "        # Ограничиваем длину слова (макс. 24 символа) для скорости\n",
    "        for j in range(max(0, i - 24), i):\n",
    "            word = text[j:i]\n",
    "            score = dp[j][0] + get_word_score(word)\n",
    "            if score > best_score:\n",
    "                best_score, best_j = score, j\n",
    "        dp[i] = (best_score, best_j)\n",
    "\n",
    "    # Восстанавливаем слова (идём назад от конца текста)\n",
    "    segments = []\n",
    "    i = n\n",
    "    while i > 0:\n",
    "        j = dp[i][1]\n",
    "        segments.append((j, i))  # добавляем границу сегмента\n",
    "        i = j\n",
    "    segments.reverse()\n",
    "\n",
    "    return segments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic_positions(text):\n",
    "    \"\"\"\n",
    "    Простая эвристика: ищет подряд идущие цифры или латинские буквы\n",
    "    После них вставляет пробел (позиция = конец последовательности)\n",
    "    \"\"\"\n",
    "    positions = []\n",
    "    for match in re.finditer(r\"[0-9a-zA-Z]+\", text):\n",
    "        positions.append(match.end())  # конец найденного блока\n",
    "    return positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predicted_positions(text):\n",
    "    \"\"\"\n",
    "    Основная функция для предсказания позиций пробелов.\n",
    "    Комбинирует:\n",
    "    - динамическое программирование (segment)\n",
    "    - частотный словарь + pymorphy2\n",
    "    - WordPiece для редких/длинных слов\n",
    "    - эвристики (цифры/латиница, заглавные буквы)\n",
    "    \"\"\"\n",
    "    segments = segment(text)\n",
    "    positions = []\n",
    "\n",
    "    for start, end in segments:\n",
    "        word = text[start:end]\n",
    "        parses = morph.parse(word)\n",
    "\n",
    "        # если слово узнаётся морфологическим анализатором или оно короткое, то ставим пробел после него\n",
    "        if parses or len(word) <= 8:\n",
    "            if len(word) >= 1: # тут изначально ставил ограничения, позже исправил штрафами\n",
    "                positions.append(end)\n",
    "        else:\n",
    "            # для длинных неизвестных слов применяем WordPiece разбиение\n",
    "            tokens = tokenizer.tokenize(word)\n",
    "            idx = start\n",
    "            for tok in tokens[:-1]:  # последний токен не даёт пробел\n",
    "                clean_tok = tok.replace(\"##\", \"\")\n",
    "                idx += len(clean_tok)\n",
    "                positions.append(idx)\n",
    "            positions.append(end)\n",
    "\n",
    "    # эвристика для цифр и латиницы\n",
    "    positions += heuristic_positions(text)\n",
    "\n",
    "    # эвристика: пробел перед одиночной заглавной буквой или новым словом с заглавной\n",
    "    for i in range(1, len(text)):\n",
    "        if text[i].isupper() and text[i-1].islower():\n",
    "            positions.append(i)\n",
    "\n",
    "    # удаляем дубликаты и сортируем\n",
    "    positions = sorted(list(set(positions)))\n",
    "\n",
    "    return positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружено 1005 строк\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "with open(\"dataset_1937770_3.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    next(f)  # пропускаем заголовок\n",
    "    for line in f:\n",
    "        line = line.rstrip(\"\\n\")\n",
    "        if not line:\n",
    "            continue  # пропускаем пустые строки\n",
    "        parts = line.split(\",\", 1)\n",
    "        if len(parts) == 2:\n",
    "            rows.append(parts)  # нормальная строка с id и текстом\n",
    "        else:\n",
    "            rows.append([parts[0], \"\"])  # если текста нет, подставляем пустую строку\n",
    "\n",
    "# создаём DataFrame для дальнейшей обработки\n",
    "task_data = pd.DataFrame(rows, columns=[\"id\", \"text_no_spaces\"])\n",
    "print(f\"Загружено {len(task_data)} строк\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_positions сформированы\n"
     ]
    }
   ],
   "source": [
    "pred_positions_list = []\n",
    "for text in task_data['text_no_spaces']:\n",
    "    # если пустая строка или не строка, то добавляем пустой список\n",
    "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "        pred_positions_list.append(\"[]\")\n",
    "    else:\n",
    "        # получаем позиции вставки пробелов\n",
    "        positions = get_predicted_positions(text)\n",
    "        # убираем последний элемент (чтобы не считать конец строки как часть разбиения)\n",
    "        pred_positions_list.append(str(positions[:-1]))\n",
    "\n",
    "# добавляем колонку с предсказанными позициями\n",
    "task_data['predicted_positions'] = pred_positions_list\n",
    "print(\"predicted_positions сформированы\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission.csv готов\n"
     ]
    }
   ],
   "source": [
    "# сохраняем результат в submission.csv для отправки на проверку\n",
    "task_data.to_csv(\"submission.csv\", index=False)\n",
    "print(\"submission.csv готов\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
